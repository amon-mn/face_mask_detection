# -*- coding: utf-8 -*-
"""ia_cnn_face_mask.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4T_lMVN5clU5NygRj1NjwufSnUEyJeF

## Bibliotecas
"""

import torch    
import os
import random
import numpy                as np
import pandas               as pd
import torch.nn.functional  as F
import matplotlib.pyplot    as plt
import seaborn              as sns
import plotly.graph_objects as go
import plotly.express       as px

import tensorflow as tf
from torch.utils.data import Dataset
from torchvision import transforms

from PIL                         import Image
from torch                       import nn, optim
from torch.nn.modules            import padding
from torch.nn.modules.activation import ReLU

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader

from torchvision                 import datasets
from plotly.subplots             import make_subplots
from google.colab                import drive

print("Versão:",torch.__version__)

"""## Leitura de dados"""

drive.mount('/content/gdrive')

! mkdir ~/.kaggle

! cp /content/gdrive/MyDrive/kaggle_api/kaggle.json ~/.kaggle/kaggle.json

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d shiekhburhan/face-mask-dataset

! unzip face-mask-dataset.zip

"""## Tratamento de dados"""

data_dir = '/content/FMD_DATASET/'

# lista todos os diretórios dentro da pasta principal
subfolders = [os.path.join(data_dir, name) for name in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, name))]

subfolders

def adicionar_prefixo_subpastas(caminho_raiz):
    # Percorrer as pastas de rótulo
    for root, dirs, files in os.walk(caminho_raiz):
        for diretorio in dirs:
            # Obter o caminho completo do diretório de rótulo
            caminho_rotulo = os.path.join(root, diretorio)
            
            # Obter o nome do rótulo
            rotulo = os.path.basename(caminho_rotulo)
            
            # Percorrer as subpastas dentro do diretório de rótulo
            for subdiretorio in os.listdir(caminho_rotulo):
                # Obter o caminho completo da subpasta
                caminho_subpasta = os.path.join(caminho_rotulo, subdiretorio)
                
                # Verificar se é um diretório
                if os.path.isdir(caminho_subpasta):
                    # Adicionar o prefixo do rótulo ao nome da subpasta
                    novo_nome = rotulo + '_' + subdiretorio
                    
                    # Renomear a subpasta
                    novo_caminho = os.path.join(caminho_rotulo, novo_nome)
                    os.rename(caminho_subpasta, novo_caminho)
                    print(f"Renomeado: {caminho_subpasta} -> {novo_caminho}")

# Exemplo de uso
caminho_pasta = data_dir  # Substitua pelo seu caminho de pasta

adicionar_prefixo_subpastas(caminho_pasta)

categorias = subfolders[0].split('/')[-1], subfolders[1].split('/')[-1], subfolders[2].split('/')[-1]
categorias = list(categorias)
categorias

class ImageFolderDataset(Dataset):
    def __init__(self, directory, transform=None):
        self.directory = directory
        self.transform = transform

        self.class_to_idx = {}
        self.imgs, self.labels = [], []
        idx = 0
        for label in os.listdir(directory):
            if label not in self.class_to_idx:
                self.class_to_idx[label] = idx
                idx += 1
            for filename in os.listdir(os.path.join(directory, label)):
                file_path = os.path.join(directory, label, filename)
                if os.path.isfile(file_path):
                    self.imgs.append(file_path)
                    self.labels.append(self.class_to_idx[label])

    def __getitem__(self, index):
        img_path = self.imgs[index]
        label = self.labels[index] + 1

        img = Image.open(img_path).convert('RGB')

        if self.transform is not None:
            img = self.transform(img)

        return img, label

    def __len__(self):
        return len(self.imgs)

transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

train_data = []
test_data = []
train_size = 0.8

# itera sobre cada subpasta
for folder in subfolders:
    dataset = ImageFolderDataset(folder, transform=transforms)
    
    # divide o conjunto de dados em treino e teste
    n = len(dataset)
    n_train = int(train_size * n)
    n_test = n - n_train
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_test])
    
    train_data.append(train_dataset)
    test_data.append(test_dataset)

# concatena os conjuntos de dados de treino e teste de cada subpasta
train_data = torch.utils.data.ConcatDataset(train_data)

test_data = torch.utils.data.ConcatDataset(test_data)

# cria os dataloaders
train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)

print(len(train_dataloader)*32, len(test_dataloader)*32)

"""## Model """

class classificador(nn.Module):
    def __init__(self):
        super(classificador, self).__init__()
        
        #Camadas convolucionais
        self.conv_layers = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        
        # Define as camadas densas
        self.dense_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(43264, 128),
            nn.ReLU(),
            nn.Linear(128, 6400),
            nn.ReLU(),
            nn.Linear(6400, 3)
            )
        
    def forward(self, x):
        x = self.conv_layers(x)
        x = self.dense_layers(x)
        return x

model = classificador()
optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)
criterion = nn.CrossEntropyLoss()

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print(device,"\n")

model.to(device)

"""## Fit"""

def train(model, dataLoader, criterion, optimizer):
    model.train()
    cumloss = 0.0
    for imagens, rotulos in dataLoader:
        imagens = imagens.to(device)
        rotulos = rotulos.to(device)

        pred = model(imagens)
        loss = criterion(pred, rotulos)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        cumloss += loss.item()

    return cumloss / len(dataLoader)


def validacao(model, dataLoader, lossfunc):
    model.eval()
    cumloss = 0.0
    
    with torch.no_grad():
        for imgs, labels in dataLoader:
            
            imgs, labels = imgs.to(device), labels.to(device)
            
            pred = model(imgs)
            
            loss = lossfunc(pred, labels)
            
            cumloss += loss.item()
            
    return cumloss / len(dataLoader)

epochs = 10
losses_treino = []
losses_teste = []

for i in range(epochs):
  loss_treino = train(model, train_dataloader, criterion, optimizer)
  losses_treino.append(loss_treino)
  if (i%2==0):
    print(f"Época: {i}; Loss Treino: {loss_treino}")

  loss_val = validacao(model, test_dataloader, criterion)
  losses_teste.append(loss_val)

"""## Metricas"""

def plot_loss(losses):
  fig = plt.figure(figsize=(13,5))
  ax = fig.gca()
  for loss_name, loss_values in losses.items():
    ax.plot(loss_values, label=loss_name)
  ax.legend(fontsize="16")
  ax.set_xlabel("Epoch", fontsize="16")
  ax.set_ylabel("Loss", fontsize="16")
  ax.set_title("Loss vs epochs", fontsize="16");

def make_confusion_matrix(model, loader, classes):
  confusion_matrix = torch.zeros(classes, classes, dtype=torch.int64)
  with torch.no_grad():
    for i ,(imagens, rotulos) in enumerate(loader):
      imagens, rotulos = imagens.to(device), rotulos.to(device) 
      output = model(imagens)
      _, pred = torch.max(output, 1)
      for j, k in zip(torch.as_tensor(rotulos, dtype=torch.int64).view(-1),
                      torch.as_tensor(pred, dtype=torch.int64).view(-1)):
        confusion_matrix[j, k] += 1
  return confusion_matrix 


def evaluate_accuracy(model, loader, classes, verbose=True):

  pred_corretos = {classname: 0 for classname in classes}
  total_pred = {classname: 0 for classname in classes}

  confusion_matrix = make_confusion_matrix(model, loader, len(classes))

  if (verbose):
    total_de_correto = 0.0
    total_de_preditos = 0.0

    for i, classname in enumerate(classes):
      correct_count = confusion_matrix[i][i].item()
      class_pred = torch.sum(confusion_matrix[i]).item()

      total_de_correto += correct_count
      total_de_preditos += class_pred

      accuracy = 100 * float(correct_count) / class_pred
      print("Acurácia da classe {}: {:.2f}".format(classname,
                                                    accuracy))
  print("Acurácia geral: {:.2f}".format(100 * total_de_correto/total_de_preditos))
  return confusion_matrix

def test(model, loader, classes):
  pred_corretos = {classname: 0 for classname in classes}
  total_pred = {classname: 0 for classname in classes}
  with torch.no_grad():
    for imagens, rotulos in loader:
      imagens, rotulos = imagens.to(device), rotulos.to(device) 
      output = model(imagens)
      _, pred = torch.max(output, 1)
      for rotulo, previsoes in zip(rotulos, pred):
        if rotulo == previsoes:
          pred_corretos[classes[rotulo]] += 1
        total_pred[classes[rotulo]] += 1


  total_de_correto = 0.0
  total_de_preditos = 0.0
  for nome_da_classe, qtd_corretos in pred_corretos.items():
    total_de_correto += qtd_corretos
    total_de_preditos += total_pred[nome_da_classe]
    acuracia = 100* float(qtd_corretos) / total_pred[nome_da_classe]
    print("Acurácia da classe {}: {:.2f}".format(nome_da_classe, acuracia))
  print("Acurácia geral: {:.2f}".format(100*total_de_correto/total_de_preditos))

grafico_de_loss = {'Loss de treino':losses_treino,
                  'Loss de teste':losses_teste}
plot_loss(grafico_de_loss)

matriz = evaluate_accuracy(model, test_dataloader, categorias)

plt.figure(figsize=(8, 7))
sns.set(font_scale=1.4)
sns.heatmap(matriz.tolist(), 
           annot=True, annot_kws={"size": 16}, fmt='d', cmap='viridis')

"""## Saving the trained model"""

# Salvando o estado do modelo CNN
torch.save(model, '/content/gdrive/MyDrive/mask_detection/model.pth')